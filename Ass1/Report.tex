\documentclass[10pt,a4paper,onecolumn]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{makeidx}
\usepackage{graphicx}
\usepackage{lmodern}
\usepackage{kpfonts}
\usepackage{booktabs}
\usepackage[left=2.5cm,right=2.5cm,top=2cm,bottom=3cm,footskip=1.7cm]{geometry}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{alltt}
\usepackage{xfrac}
\usepackage{tikz}


% change headers and footers (package fancyhdr)
\pagestyle{fancy}
\fancyhf[HL]{}
%fancyhf[HR]{Utrecht University}
\fancyhf[FR]{\thepage ~of \pageref{LastPage}}
\fancyhf[FC]{INFO-EA Evolutionary Computing \\ Utrecht University}
\fancyhf[FL]{Julius van Dis\\ 4038010 \and \\ Myrna van de Burgwal \\ 3296725}



% document info
\title{Analysis on the performance of a genetic algorithm\\ \textit{INFO-EA Evolutionary Computing}}
\author{Julius \textsc{van Dis} \\ 4038010 \\vandis.j@gmail.com
\and Myrna \textsc{van de Burgwal} \\ 3296725 \\myrnavandeburgwal@gmail.com}

%%%% %%% START DOCUMENT %%% %%%
\begin{document}
\maketitle
\thispagestyle{empty}
%
\section*{}
%Introduction blaat
%What have we done?
We have written a program that allows us to gain insight in the conversion behaviour of genetic algorithms. In particular, we looked at binary bitstrings, with a length of $l=100$ . After generating random bits, we tried to find a global optimum, which consists of only ones. Based on four different fitness functions, our goal was to compare several parameter settings and find the binary vector that maximises the fitness function. Since the result depends on the initial population, each type of settings was run fifty times after which we counted how many times a global optimum was found.
....
....
Details on the assignment can be find in the assignment PDF.

As an additional research question, we performed some experiments to find out more about the influence of the selection procedure. Our initial program only considered tournament selection with size 1 or 2, where 1 is random selection. We extended this selection procedure with options for ... and  ... .. . The details can be found in Section~\ref{sec:research}.

\section{Explanation of implementation}
The genetic algorithm was implemented in java, using the Eclipse SDK. The reason for this was merely from a practical point of view, since both authors are familiar with the possibilities of java, and object oriented programming would allow us to implement the program in a modular way, which allows for extensions to be implemented easily.

%General explanation
If one looks at the written program, one will see that it consists of three files. One file, \textit{main.java} that contains the main, where all the parameter settings for the problem are given and from which the actual genetic algorithm is called. Then there is \textit{Ga.java}, which contains the algorithm itself, and third there is \textit{Solution.java}, which contains a helper class \textit{Solution}, in which the properties of a certain solution are kept, as well as where the calculations for the fitness are done. The subsequent sections discuss these three files in more detail.

%explanation per file
\subsection{Main.java}
In the main file of the program, the most essential part is the selection of the right parameter settings to run the program on. Some initial experiments, as well as some comparisons with comparable research, we were able to narrow down some of the parameter settings (see Section~\ref{sec:results}). 

\textit{Main} makes a call to \textit{Ga}, with the request to run a genetic algorithm with the given parameter settings. The results, which comes in an ArrayList are then further processed and presented to the user.


\subsection{Ga.java}
The second file is the \textit{Ga.java} file with the genetic algorithm (Ga) class. This contains the genetic algorithm itself. Upon the initialisation of an instance of the class, the parameter settings which are given are set. Next, the algorithm can be ran by calling \textit{runGa}. This will execute the genetic algorithm, with the following steps.

\begin{verbatim}
1 run for 50 times
2    generate a random population with solution length 100
3    WHILE population has changed recently 
       AND the max fitness is not achieved yet DO
4       select two candidates (tournament selection with size 1 or 2)
5       perform crossover or mutation on two candidates
6       select best solution of candidates
7       if best solution is better than worst of the population
8         perform sorted insert
9    add WIN/LOSE, evaluation# and average fitness to 'answer' matrix
10 return 'answer' matrix
\end{verbatim}

A couple of remarks: Upon generation of the population, the population is sorted as well based on their fitness (descending). When tournament selection is called, this will always return 2 solutions (i.e. it will perform the tournament twice). 

Crossover can be performed in multiple ways. Independent of the fitness function, uniform crossover and 2point crossover can take place. In case the fitness function is a trap function (either deceptive or non-deceptive), the 2point crossover is such that the left and right point are always a multitude of 4 apart. This means that when there is a tight linkage, only entire subfunctions can be exchanged.

The mutation is performed as proposed. First, the amount of mutations is determined by simulating fair coin flipping. Then, the locations which will mutate are selected, and subsequently they are mutated.

Then, after mutation or crossover, the best child is selected. If its fitness is equal or larger then the fitness of the worst solution in the population, a sorted insert is performed. Then, the iteration starts over, and continues until the WHILE condition does not hold any more (when global maximum is reached or when the populations has not changed for a long time). And finally, the whole process of finding the global optimum is repeated for a certain parameter set 50 times.

\subsection{Solution.java}
As a helper class, \textit{Solution} is the last file of the program. The properties of a solution are saved in an instance of this class. This means that the id, the bitstring itself, the fitness function, the fitness, and the linkage are set upon initialisation. It also contains methods to calculate the fitness of a solution, as well as the maximum achievable fitness for a bitstring evaluated under a certain fitness function.


\section{Counting ones}

\subsection{Uniformly Scaled Counting Ones Function}

For the uniformly scaled counting ones function, we have run our program for population sizes between 100 and 1000.
It turned out that a global optimum was easily found if the algorithm included mutation (i.e. if Pc equals 0 or 0.5).
The population size needed to be large until global optimum were found when Pc=1.
This is probably the case, because using crossover on a small population size can lead to convergence to a local optimum.
Mutation continuously caused changes in the population, whereas crossover could result into a specific bit being zero for the entire population.
For Pc=0 and Pc=0.5 a population size of respectively 150 and 200 would have been enough, wherease Pc=1 would require much more solutions. \\

Another interesting observation is that a tournament size of 1 led to more found global optima than a tournament size of 2.
We expected it to be the other way around, since a size of 2 would remove all bad solutions.
Probably the selection pressure was too strong and an early convergence led to local optima.\\

A final aspect that we have compared is the types of crossover. For all kinds of parameter settings, uniform crossover gave better results than two-point crossover.
For Pc=0.5 this difference was small, but for Pc=1 it was quite significant. This is probably the case, since in small population sizes entire segments are being switched.

We also discovered some interesting results of the duration before a global optimum was found. 
We have computed the average number of generations between all 50 runs for the parameter settings.
It turned out that this number was much higher for Pc=0 than for Pc=0.5 and Pc=1 (with Pc=1 being a little faster than Pc=0.5).
We assume that crossover leads to a quick convergence, with either a global optimum or a local optimum as a result.
A tournament size of 1 gave almost twice as many generations as a size of 2.
This is also probably the case since a size of 2 made the fitnesses convergence very quickly.
The differences of both comparisons were larger when the population size was larger.
Uniform crossover had less generations than two-point crossover.

Below we show the results in graphs. The x-axis represents the population sizes, whereas the y-axis stands for the ratio of global optima being found (with a threshold of 0.98).

\begin{figure}
    \centering
    %\includegraphics[width=0.8\textwidth]{Fit1Pc0.png}
    \caption{The results for a crossover probality of 0. Comparing tournament sizes (s) of 1 and 2}
    %\label{}
\end{figure}

\begin{figure}
    \centering
    %\includegraphics[width=0.8\textwidth]{Fit1Pc05.png}
    \caption{Results for Pc=0.5. Comparing tournament sizes (s) and the two different crossover types}
    %\label{}
\end{figure}

\begin{figure}
    \centering
    %\includegraphics[width=0.8\textwidth]{Fit1Pc1.png}
    \caption{Results for Pc=1. Comparing tournament sizes (s) and the two different crossover types}
    %\label{}
\end{figure}

\subsection{Linearly Scaled Counting Ones Function}

Just like in the first fitness function, we have run our program for the linearly scaled counting ones function with population sizes between 100 and 1000.
The results were similar to the results of the former fitness function.
Again, mutation led to much better results than crossover.
When Pc was equal to 0, almost always a global optimum was found. Pc=0.5 gave slightly worse results.
But with Pc=1, a large population size was needed to find global optima.\\

More global optima were found when the tournament size was equal to 1 (instead of 2), presumably due to a too high selection pressure of a size of 2. Also, uniform crossover could find a global optimum more often than two-point crossover.
This difference is even larger than in the first fitness function.

The differences in the number of generations were similar to those of the first fitness function.
Crossover was quicker than mutation, a tournament size of 2 was quicker than a size of 1, and uniform crossover was quicker than two-point crossover.

The results can be seen below in graphs (again comparing ratios of found global optima on population sizes).

\begin{figure}
    \centering
    %\includegraphics[width=0.8\textwidth]{Fit2Pc0.png}
    \caption{The results for a crossover probality of 0. Comparing tournament sizes (s) of 1 and 2}
    %\label{}
\end{figure}

\begin{figure}
    \centering
    %\includegraphics[width=0.8\textwidth]{Fit2Pc05.png}
    \caption{Results for Pc=0.5. Comparing tournament sizes (s) and the two different crossover types}
    %\label{}
\end{figure}

\begin{figure}
    \centering
    %\includegraphics[width=0.8\textwidth]{Fit2Pc1.png}
    \caption{Results for Pc=1. Comparing tournament sizes (s) and the two different crossover types}
    %\label{}
\end{figure}

\subsection{Conclusion of CO and SCO}

Comparing both fitness functions, we found that the results of the uniformly scaled fitness function were slightly better than those of the linearly scaled function.
This difference is more significant when Pc=1.
This could be the case, because the function is linearly scaled and the first bits have very low influence on the fitness value.
In all cases, the uniformly scaled function was faster than the linearly scaled function.
We assumed that this has also to do with the fact that the first bits are hardly taken into account when the fitness is computed and it takes longer before a global optimum is found. \\

Overall we observed that mutation resulted into more global optima than crossover.
In general tournament selection will improve the fitnesses of a population, but the population size should be large enough to prevent local optima.
Since global optima can be found with small population sizes, tournament selection is not very helpful in these cases.
For both functions, uniform crossover gave better results and also needed less generations than two-point crossover. \\

We have seen that a global optimum is always found if we use mutation with a population size of more than 100. If we take a tournament size of 1, an even smaller population size is needed.
It is also possible to find global optima with a small population size when we use crossover, only it is necessary to take a tournament size of 1.

\section{Trap functions}

\subsection{Deceptive Trap Function}
Since we had soon discovered that the deceptive trap function performed very bad, we increased our population sizes. We ran our program for sizes between 500 and 2500. When it turned out that the results were not that good, we also tried sizes above 2500, but we did not find much improvement. \\
Although mutation gave good results for the counting ones functions, it never found a global optimum when using the deceptive trap function. For Pc=0.5, the results were better, but a large population size was required. Pc=1 gave the best results. We can therefore conclude that crossover performs better than mutation when a trap function is used. This is probably the case, because 3 adjacent ones are punished, while we need 4 adjacent ones and crossover has a higher probability of keeping 4 adjacent ones than mutation. \\
Surprinsingly, uniform crossover never gave a global optimum, but two-point crossover did. The latter works better since the fitness function takes groups of four bits and more inheritance is kept with two-point crossover. However, if random linkage was used, a global optimum was never found either. This is because the inheritance is lost when the bits are randomly linked. For tight linkage a tournament size of 1 performed better than a size of 2. Again, this could be the case since a size of 2 leads to early convergence. 
Below the results are shown in graphs (the x-axis represents the population sizes and the y-axis the ratio of global optima being found).
\begin{figure}
    \centering
    %\includegraphics[width=0.8\textwidth]{Fit3Pc0.png}
    \caption{The results for a crossover probality of 0. Comparing tournament sizes (s) of 1 and 2}
    %\label{}
\end{figure}

\begin{figure}
    \centering
    %\includegraphics[width=0.8\textwidth]{Fit3Pc05.png}
    \caption{Results for Pc=0.5. Comparing tournament sizes (s) and the two different crossover types}
    %\label{}
\end{figure}

\begin{figure}
    \centering
    %\includegraphics[width=0.8\textwidth]{Fit3Pc1.png}
    \caption{Results for Pc=1. Comparing tournament sizes (s) and the two different crossover types}
    %\label{}
\end{figure}

\subsection{Non-deceptive Trap Function}
Naturally, the non-deceptive trap function performed much better than the deceptive one. We used population sizes between 500 and 2000. Again, crossover performed better than mutation. Since Pc=0 gave very bad results, we ran the program again with population sizes until 6000. Still, hardly any global optima were found. A tournament size of 1 performed better than a size of 2. \\
The performance of Pc=0.5 was quite good. For two-point crossover a global optimum was always found.  Again, uniform crossover gave worse results, which has to do with the inheritance of the subgroups of the bitstrings. Very interesting was the fact that random linkage performed better than tight linkage, which was a very surprinsing observation. For Pc=1 this was the other way around. For both Pc=0.5 and Pc=1 a tournament size of 1 performed better than a size of 2.
The results can be seen below in graphs (x-axis = population sizes, y-axis = ratio).

\begin{figure}
    \centering
    %\includegraphics[width=0.8\textwidth]{Fit4Pc0.png}
    \caption{The results for a crossover probality of 0. Comparing tournament sizes (s) of 1 and 2}
    %\label{}
\end{figure}

\begin{figure}
    \centering
    %\includegraphics[width=0.8\textwidth]{Fit4Pc05.png}
    \caption{Results for Pc=0.5. Comparing tournament sizes (s) and the two different crossover types}
    %\label{}
\end{figure}

\begin{figure}
    \centering
    %\includegraphics[width=0.8\textwidth]{Fit4Pc1.png}
    \caption{Results for Pc=1. Comparing tournament sizes (s) and the two different crossover types}
    %\label{}
\end{figure}

\subsection{Conclusion of deceptive functions}

We observed that random linkage performed very slow. Our algorithm to link the bits randomly was quite efficient, but still it took much longer to compute these results than for tight linkage. \\
As was expected, the non-deceptive function gave better results than the deceptive function. Again, a tournament size of 1 turned out to be better than a size of 2, because of too early convergence of the latter. Unlike the counting ones functions, the deceptive functions performed better with crossover than mutation and better with two-point crossover than uniform crossover. This has to do with the inheritance of the bit groups during the fitness evaluations.
\section{Results} \label{sec:results}
including time and stuff
and also the required plots

\section{Influence of different selection method on the performance}
\label{sec:research}
As an additional research question we have decided to extend our current implementation of the algorithm with some extra selection method. Currently only tournament selection with size 2, and random selection is done. We would like to see the effects of applying tournament selection with sizes (SIZES????) and also with roulette wheel selection, and truncation selection.

\subsection{Additional implementation explained}

\subsection{Results and findings}



\end{document}
