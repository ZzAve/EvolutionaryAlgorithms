\documentclass[10pt,a4paper,onecolumn]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{makeidx}
\usepackage{graphicx}
\usepackage{lmodern}
\usepackage{kpfonts}
\usepackage{booktabs}
\usepackage[left=2.5cm,right=2.5cm,top=2cm,bottom=3cm,footskip=1.7cm]{geometry}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{alltt}
\usepackage{xfrac}
\usepackage{tikz}


% change headers and footers (package fancyhdr)
\pagestyle{fancy}
\fancyhf[HL]{}
%fancyhf[HR]{Utrecht University}
\fancyhf[FR]{\thepage ~of \pageref{LastPage}}
\fancyhf[FC]{INFO-EA Evolutionary Computing \\ Utrecht University}
\fancyhf[FL]{Julius van Dis\\ 4038010 \and \\ Myrna van de Burgwal \\ 3296725}



% document info
\title{Analysis on the performance of a genetic algorithm\\ \textit{INFO-EA Evolutionary Computing}}
\author{Julius \textsc{van Dis} \\ 4038010 \\vandis.j@gmail.com
\and Myrna \textsc{van de Burgwal} \\ 3296725 \\myrnavandeburgwal@gmail.com}

%%%% %%% START DOCUMENT %%% %%%
\begin{document}
\maketitle
\thispagestyle{empty}
%
\section*{}
%Introduction blaat
%What have we done?
We have written a program that allows us to gain insight in the conversion behaviour of genetic algorithms. In particular, we looked at binary bitstrings, with a length of $l=100$ . After generating random bits, we tried to find a global optimum, which consists of only ones. Based on four different fitness functions, our goal was to compare several parameter settings and find the binary vector that maximises the fitness function. Since the result depends on the initial population, each type of settings was run fifty times after which we counted how many times a global optimum was found.
....
....
Details on the assignment can be find in the assignment PDF.

As an additional research question, we performed some experiments to find out more about the influence of the selection procedure. Our initial program only considered tournament selection with size 1 or 2, where 1 is random selection. We extended this selection procedure with options for ... and  ... .. . The details can be found in Section~\ref{sec:research}.

\section{Explanation of implementation}
The genetic algorithm was implemented in java, using the Eclipse SDK. The reason for this was merely from a practical point of view, since both authors are familiar with the possibilities of java, and object oriented programming would allow us to implement the program in a modular way, which allows for extensions to be implemented easily.

%General explanation
If one looks at the written program, one will see that it consists of three files. One file, \textit{main.java} that contains the main, where all the parameter settings for the problem are given and from which the actual genetic algorithm is called. Then there is \textit{Ga.java}, which contains the algorithm itself, and third there is \textit{Solution.java}, which contains a helper class \textit{Solution}, in which the properties of a certain solution are kept, as well as where the calculations for the fitness are done. The subsequent sections discuss these three files in more detail.

%explanation per file
\subsection{Main.java}
In the main file of the program, the most essential part is the selection of the right parameter settings to run the program on. Some initial experiments, as well as some comparisons with comparable research, we were able to narrow down some of the parameter settings (see Section~\ref{sec:results}). 

\textit{Main} makes a call to \textit{Ga}, with the request to run a genetic algorithm with the given parameter settings. The results, which comes in an ArrayList are then further processed and presented to the user.


\subsection{Ga.java}
The second file is the \textit{Ga.java} file with the genetic algorithm (Ga) class. This contains the genetic algorithm itself. Upon the initialisation of an instance of the class, the parameter settings which are given are set. Next, the algorithm can be ran by calling \textit{runGa}. This will execute the genetic algorithm, with the following steps.

\begin{verbatim}
1 run for 50 times
2    generate a random population with solution length 100
3    WHILE population has changed recently 
       AND the max fitness is not achieved yet DO
4       select two candidates (tournament selection with size 1 or 2)
5       perform crossover or mutation on two candidates
6       select best solution of candidates
7       if best solution is better than worst of the population
8         perform sorted insert
9    add WIN/LOSE, evaluation# and average fitness to 'answer' matrix
10 return 'answer' matrix
\end{verbatim}

A couple of remarks: Upon generation of the population, the population is sorted as well based on their fitness (descending). When tournament selection is called, this will always return 2 solutions (i.e. it will perform the tournament twice). 

Crossover can be performed in multiple ways. Independent of the fitness function, uniform crossover and 2point crossover can take place. In case the fitness function is a trap function (either deceptive or non-deceptive), the 2point crossover is such that the left and right point are always a multitude of 4 apart. This means that when there is a tight linkage, only entire subfunctions can be exchanged.

The mutation is performed as proposed. First, the amount of mutations is determined by simulating fair coin flipping. Then, the locations which will mutate are selected, and subsequently they are mutated.

Then, after mutation or crossover, the best child is selected. If its fitness is equal or larger then the fitness of the worst solution in the population, a sorted insert is performed. Then, the iteration starts over, and continues until the WHILE condition does not hold any more (when global maximum is reached or when the populations has not changed for a long time). And finally, the whole process of finding the global optimum is repeated for a certain parameter set 50 times.

\subsection{Solution.java}
As a helper class, \textit{Solution} is the last file of the program. The properties of a solution are saved in an instance of this class. This means that the id, the bitstring itself, the fitness function, the fitness, and the linkage are set upon initialisation. It also contains methods to calculate the fitness of a solution, as well as the maximum achievable fitness for a bitstring evaluated under a certain fitness function.


\section{Counting ones}

\subsection{Uniformly Scaled Counting Ones Function}

For the uniformly scaled counting ones function, we have run our program for population sizes between 100 and 1000.
It turned out that a global optimum was easily found if the algorithm included mutation (i.e. if Pc equals 0 or 0.5).
The population size needed to be large until global optimum were found when Pc=1.
This is probably the case, because using crossover on a small population size can lead to convergence to a local optimum.
Mutation continuously caused changes in the population, whereas crossover could result into a specific bit being zero for the entire population.
For Pc=0 and Pc=0.5 a population size of respectively 150 and 200 would have been enough, wherease Pc=1 would require much more solutions. \\

Another interesting observation is that a tournament size of 1 led to more found global optima than a tournament size of 2.
We expected it to be the other way around, since a size of 2 would remove all bad solutions.
Probably the selection pressure was too strong and an early convergence led to local optima.\\

A final aspect that we have compared is the types of crossover. For all kinds of parameter settings, uniform crossover gave better results than two-point crossover.
For Pc=0.5 this difference was small, but for Pc=1 it was quite significant. This is probably the case, since in small population sizes entire segments are being switched.

We also discovered some interesting results of the duration before a global optimum was found. 
We have computed the average number of generations between all 50 runs for the parameter settings.
It turned out that this number was much higher for Pc=0 than for Pc=0.5 and Pc=1 (with Pc=1 being a little faster than Pc=0.5).
We assume that crossover leads to a quick convergence, with either a global optimum or a local optimum as a result.
A tournament size of 1 gave almost twice as many generations as a size of 2.
This is also probably the case since a size of 2 made the fitnesses convergence very quickly.
The differences of both comparisons were larger when the population size was larger.
Uniform crossover had less generations than two-point crossover.


\subsection{Linearly Scaled Counting Ones Function}

Just like in the first fitness function, we have run our program for the linearly scaled counting ones function with population sizes between 100 and 1000.
The results were similar to the results of the former fitness function.
Again, mutation led to much better results than crossover.
When Pc was equal to 0, almost always a global optimum was found. Pc=0.5 gave slightly worse results.
But with Pc=1, a large population size was needed to find global optima.\\

More global optima were found when the tournament size was equal to 1 (instead of 2), presumably due to a too high selection pressure of a size of 2. Also, uniform crossover could find a global optimum more often than two-point crossover.
This difference is even larger than in the first fitness function.

The differences in the number of generations were similar to those of the first fitness function.
Crossover was quicker than mutation, a tournament size of 2 was quicker than a size of 1, and uniform crossover was quicker than two-point crossover.

\subsection{Conclusion of CO and SCO}

Comparing both fitness functions, we found that the results of the uniformly scaled fitness function were slightly better than those of the linearly scaled function.
This difference is more significant when Pc=1.
This could be the case, because the function is linearly scaled and the first bits have very low influence on the fitness value.
In all cases, the uniformly scaled function was faster than the linearly scaled function.
We assumed that this has also to do with the fact that the first bits are hardly taken into account when the fitness is computed and it takes longer before a global optimum is found. \\

Overall we observed that mutation resulted into more global optima than crossover.
In general tournament selection will improve the fitnesses of a population, but the population size should be large enough to prevent local optima.
Since global optima can be found with small population sizes, tournament selection is not very helpful in these cases. \\

We have seen that a global optimum is always found if we use mutation with a population size of more than 100. If we take a tournament size of 1, an even smaller population size is needed.
It is also possible to find global optima with a small population size when we use crossover, only it is necessary to take a tournament size of 1.


\section{Results} \label{sec:results}
including time and stuff
and also the required plots

\section{Influence of different selection method on the performance}
\label{sec:research}
As an additional research question we have decided to extend our current implementation of the algorithm with some extra selection method. Currently only tournament selection with size 2, and random selection is done. We would like to see the effects of applying tournament selection with sizes (SIZES????) and also with roulette wheel selection, and truncation selection.

\subsection{Additional implementation explained}

\subsection{Results and findings}



\end{document}
